# 

## 9.1 RNN의 이해

**정의**: 순차 데이터 처리를 위한 순환 구조 신경망

### RNN 기본 구조

```
시간 전개 (Unrolling)
─────────────────────
      h₀      h₁      h₂      h₃
       ↓       ↓       ↓       ↓
x₀ → [RNN] → [RNN] → [RNN] → [RNN] → 출력
       ↑       ↑       ↑       ↑
      h₀  →   h₁  →   h₂  →   h₃
      
hₜ = tanh(Wₓₕ·xₜ + Wₕₕ·hₜ₋₁ + b)
```

### 주요 특징

|항목|설명|
|---|---|
|순환 연결|이전 시점의 은닉 상태를 현재 입력과 결합|
|가변 길이|다양한 길이의 시퀀스 처리 가능|
|파라미터 공유|모든 시점에서 동일한 가중치 사용|

### RNN의 한계

|문제|설명|
|---|---|
|기울기 소실|긴 시퀀스에서 초기 정보 손실|
|기울기 폭발|그래디언트가 기하급수적 증가|
|장기 의존성|멀리 떨어진 정보 학습 어려움|

### LSTM (Long Short-Term Memory)

```
LSTM 셀 구조
────────────
        cₜ₋₁ ──────────────────→ cₜ
              ↑     ↑        ↑
             (×)   (+)      (×)
              ↑     ↑        ↑
         forget  input    output
          gate   gate      gate
              ↖   ↑   ↗
                 xₜ, hₜ₋₁
```

|게이트|역할|
|---|---|
|Forget Gate|이전 정보 중 버릴 것 결정|
|Input Gate|새 정보 중 저장할 것 결정|
|Output Gate|출력할 정보 결정|
|Cell State|장기 기억 저장|

### GRU (Gated Recurrent Unit)

```
GRU 구조 (LSTM 간소화)
─────────────────────
Reset Gate: 이전 정보 리셋 정도
Update Gate: 정보 갱신 비율

LSTM 대비: 게이트 2개, 파라미터 적음, 속도 빠름
```

### RNN 변형 구조

|구조|입출력|용도|
|---|---|---|
|One-to-Many|1 → N|이미지 캡셔닝|
|Many-to-One|N → 1|감성 분석, 분류|
|Many-to-Many|N → N|번역, 품사 태깅|

## 9.2 언어 모델의 이해

### 9.1.1 자연어 처리 (NLP)

**정의**: 컴퓨터가 인간 언어를 이해하고 생성하는 기술

#### 주요 태스크

|태스크|설명|예시|
|---|---|---|
|텍스트 분류|문서 카테고리 분류|스팸 필터, 감성 분석|
|개체명 인식 (NER)|고유명사 추출|인명, 지명, 기관명|
|기계 번역|언어 간 번역|영→한 번역|
|질의응답 (QA)|질문에 답변|챗봇, 검색|
|텍스트 생성|새로운 텍스트 생성|GPT, 요약|

### 9.1.2 텍스트 전처리

#### 전처리 파이프라인

```
원본 텍스트
    ↓
정제 (특수문자, HTML 제거)
    ↓
토큰화 (단어/서브워드 분리)
    ↓
정규화 (소문자, 어간 추출)
    ↓
불용어 제거
    ↓
수치화 (인덱싱, 임베딩)
```

#### 토큰화 방법

|방법|설명|예시|
|---|---|---|
|단어 토큰화|공백/구두점 기준 분리|"I love NLP" → ["I", "love", "NLP"]|
|서브워드|BPE, WordPiece|"playing" → ["play", "##ing"]|
|문자 토큰화|문자 단위 분리|"cat" → ["c", "a", "t"]|

#### 텍스트 수치화

|방법|설명|
|---|---|
|One-hot|희소 벡터, 단어 수 차원|
|TF-IDF|단어 중요도 반영|
|Word2Vec|밀집 벡터, 의미 유사성|
|Embedding Layer|학습 가능한 임베딩|

### 9.1.3 Seq2Seq

**정의**: 인코더-디코더 구조로 시퀀스를 시퀀스로 변환

```
Seq2Seq 구조
────────────
입력: "I love you"

[Encoder]
 I → love → you → [Context Vector]
                        ↓
[Decoder]        [Context Vector]
                        ↓
                나는 → 너를 → 사랑해 → <EOS>

출력: "나는 너를 사랑해"
```

|구성요소|역할|
|---|---|
|Encoder|입력 시퀀스를 고정 길이 벡터로 압축|
|Context Vector|입력 정보를 담은 벡터|
|Decoder|컨텍스트 벡터로부터 출력 시퀀스 생성|

**한계**: 긴 문장에서 정보 손실 (병목 현상)

### 9.1.4 Attention

**정의**: 디코더가 인코더의 모든 출력을 참조하여 중요 부분에 집중

```
Attention 메커니즘
─────────────────
         h₁    h₂    h₃    h₄   ← 인코더 출력
          ↓     ↓     ↓     ↓
         α₁    α₂    α₃    α₄   ← 가중치 (합=1)
          ↓     ↓     ↓     ↓
        ┌──────────────────────┐
        │   가중합 (Context)    │
        └──────────────────────┘
                  ↓
             디코더 출력
```

#### Attention 계산

```
1. Score 계산: score(sₜ, hᵢ)
2. Softmax로 가중치: αᵢ = softmax(scoreᵢ)
3. Context Vector: c = Σαᵢ·hᵢ
4. 출력 생성: concat(sₜ, c) → output
```

|장점|설명|
|---|---|
|병목 해결|모든 인코더 출력 참조|
|해석 가능|어텐션 가중치로 시각화|
|장거리 의존성|멀리 떨어진 정보도 직접 참조|

### 9.1.5 Transformer

**정의**: RNN 없이 Attention만으로 구성된 구조 (2017, "Attention is All You Need")

```
Transformer 구조
────────────────
입력 임베딩 + 위치 인코딩
           ↓
┌─────────────────────┐
│      Encoder        │ × N
│  ┌───────────────┐  │
│  │Multi-Head Attn│  │
│  └───────────────┘  │
│         ↓           │
│  ┌───────────────┐  │
│  │  Feed Forward │  │
│  └───────────────┘  │
└─────────────────────┘
           ↓
┌─────────────────────┐
│      Decoder        │ × N
│  ┌───────────────┐  │
│  │Masked MH Attn │  │
│  └───────────────┘  │
│         ↓           │
│  ┌───────────────┐  │
│  │Cross Attention│  │
│  └───────────────┘  │
│         ↓           │
│  ┌───────────────┐  │
│  │  Feed Forward │  │
│  └───────────────┘  │
└─────────────────────┘
           ↓
        출력
```

#### 핵심 구성요소

|구성요소|설명|
|---|---|
|Self-Attention|시퀀스 내 모든 위치 간 관계 학습|
|Multi-Head|여러 관점에서 어텐션 수행|
|Positional Encoding|위치 정보 주입 (순서 표현)|
|Feed Forward|비선형 변환|
|Residual + LayerNorm|학습 안정화|

#### Self-Attention 계산

```
Q (Query), K (Key), V (Value)

Attention(Q, K, V) = softmax(QKᵀ / √dₖ) · V
```

|장점|설명|
|---|---|
|병렬 처리|RNN과 달리 모든 위치 동시 계산|
|장거리 의존성|거리에 관계없이 직접 연결|
|확장성|대규모 모델 학습 가능|

### 9.1.6 최신 자연어 처리 핵심 기술 요소

#### 대표 모델 계보

```
Transformer (2017)
      ↓
┌─────┴─────┐
↓           ↓
BERT       GPT
(인코더)    (디코더)
↓           ↓
RoBERTa    GPT-2/3/4
ALBERT     ChatGPT
↓
T5, BART (인코더-디코더)
```

#### 주요 모델 비교

|모델|구조|특징|용도|
|---|---|---|---|
|BERT|인코더|양방향, MLM|분류, NER, QA|
|GPT|디코더|단방향, 자기회귀|텍스트 생성|
|T5|인코더-디코더|Text-to-Text|범용|
|LLaMA|디코더|효율적 오픈소스|생성, 범용|

#### 핵심 기술 요소

|기술|설명|
|---|---|
|사전학습 (Pre-training)|대규모 데이터로 언어 지식 학습|
|미세조정 (Fine-tuning)|특정 태스크에 맞게 추가 학습|
|프롬프트 엔지니어링|입력 설계로 모델 제어|
|In-context Learning|예시만으로 태스크 수행 (Few-shot)|
|RLHF|인간 피드백 기반 강화학습|
|LoRA/PEFT|효율적 파라미터 미세조정|

#### 임베딩 발전

```
One-hot → Word2Vec → ELMo → BERT 임베딩
(정적)    (정적)    (문맥)   (문맥, 양방향)
```

|임베딩|특징|
|---|---|
|Word2Vec|정적 임베딩, 동음이의어 구분 불가|
|ELMo|문맥 기반, BiLSTM|
|BERT|양방향 문맥, Transformer|