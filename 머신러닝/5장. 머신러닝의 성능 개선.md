## 5.1 머신러닝에서 성능이란?

### 성능의 정의

|관점|설명|
|---|---|
|일반화 성능|새로운 데이터에 대한 예측 능력|
|학습 성능|훈련 데이터에 대한 적합도|

### 성능 저하 원인

|문제|원인|해결 방향|
|---|---|---|
|과대적합|모델 복잡도↑, 데이터↓|정규화, 데이터 증강, 단순화|
|과소적합|모델 복잡도↓, 특성 부족|모델 복잡도↑, 특성 공학|
|데이터 품질|노이즈, 결측치, 이상치|전처리 강화|
|데이터 불균형|클래스 비율 불균형|샘플링, 가중치 조정|

### 성능 개선 전략

```
성능 개선
├── 데이터 관점 : 품질 향상, 증강, 특성 공학
├── 모델 관점 : 알고리즘 선택, 앙상블
├── 학습 관점 : 하이퍼파라미터 튜닝, 조기 종료
└── 평가 관점 : 교차검증, 적절한 지표 선택
```

## 5.2 교차검증 (Cross-Validation)

**목적**: 데이터 분할에 따른 성능 변동 최소화, 일반화 성능 추정

### K-Fold 교차검증

```
Fold 1: [Test] [Train] [Train] [Train] [Train]
Fold 2: [Train] [Test] [Train] [Train] [Train]
Fold 3: [Train] [Train] [Test] [Train] [Train]
Fold 4: [Train] [Train] [Train] [Test] [Train]
Fold 5: [Train] [Train] [Train] [Train] [Test]

최종 성능 = 각 Fold 성능의 평균
```

### 교차검증 종류

|방법|설명|적용|
|---|---|---|
|K-Fold|K개 폴드로 분할|일반적 사용|
|Stratified K-Fold|클래스 비율 유지|불균형 데이터|
|Leave-One-Out|1개씩 테스트|소규모 데이터|
|Time Series Split|시간 순서 유지|시계열 데이터|
|Group K-Fold|그룹 단위 분할|그룹 의존성 존재 시|

### 장단점

|장점|단점|
|---|---|
|모든 데이터 학습/평가|계산 비용 증가|
|안정적 성능 추정|K 선택 필요|
|과적합 탐지 용이||

## 5.3 하이퍼파라미터 튜닝

**정의**: 모델 학습 전 설정하는 값의 최적화

### 파라미터 vs 하이퍼파라미터

|구분|파라미터|하이퍼파라미터|
|---|---|---|
|결정 시점|학습 중|학습 전|
|결정 주체|모델|사용자|
|예시|가중치, 편향|학습률, 트리 깊이|

### 튜닝 방법

|방법|설명|특징|
|---|---|---|
|Grid Search|모든 조합 탐색|완전 탐색, 시간 오래 걸림|
|Random Search|무작위 조합 탐색|효율적, 넓은 범위 탐색|
|Bayesian Optimization|이전 결과 기반 탐색|효율적, 복잡한 구현|
|Halving Grid/Random|자원 점진 할당|빠른 수렴|

### 주요 모델별 하이퍼파라미터

|모델|주요 파라미터|
|---|---|
|Decision Tree|max_depth, min_samples_split, min_samples_leaf|
|Random Forest|n_estimators, max_depth, max_features|
|XGBoost/LightGBM|learning_rate, n_estimators, max_depth, reg_alpha/lambda|
|SVM|C, kernel, gamma|
|KNN|n_neighbors, weights, metric|

### 튜닝 예시 (Grid Search)

python

````python
param_grid = {
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1]
}
# 총 3 × 2 × 2 = 12 조합 탐색
```

## 5.4 특성 공학 (Feature Engineering)

**정의**: 원본 데이터로부터 유용한 특성 생성/선택/변환

### 특성 공학 유형
```
특성 공학
├── 특성 생성 : 새로운 특성 만들기
├── 특성 변환 : 기존 특성 형태 변경
├── 특성 선택 : 유용한 특성만 선별
└── 특성 추출 : 차원 축소로 새 특성 생성
```

### 특성 생성 기법

| 기법 | 설명 | 예시 |
|------|------|------|
| 조합 | 특성 간 연산 | 가격/면적 = 평당가 |
| 분해 | 하나를 여러 개로 | 날짜 → 년, 월, 일, 요일 |
| 집계 | 그룹별 통계 | 고객별 평균 구매액 |
| 구간화 | 연속형 → 범주형 | 나이 → 연령대 |
| 다항 특성 | 제곱, 교차항 | x², x₁×x₂ |

### 특성 선택 방법

| 방법 | 설명 |
|------|------|
| Filter | 통계적 기준 (상관계수, 분산) |
| Wrapper | 모델 성능 기반 (RFE, 전진/후진 선택) |
| Embedded | 모델 내장 (Lasso, 트리 기반 중요도) |

### 특성 중요도 확인

| 방법 | 적용 모델 |
|------|----------|
| feature_importances_ | 트리 기반 모델 |
| coef_ | 선형 모델 |
| Permutation Importance | 모든 모델 |
| SHAP | 모든 모델 |

## 5.5 학습의 이른 종료 (Early Stopping)

**정의**: 검증 성능이 개선되지 않으면 학습 조기 중단

### 작동 원리
```
성능
 ↑
 │    ╭──────── 검증 손실
 │   ╱  ╲
 │  ╱    ╲___________  ← 과적합 시작
 │ ╱
 │╱__________ 훈련 손실
 └─────────────────────→ Epoch
        ↑
    Early Stop 지점
````

### 주요 파라미터

|파라미터|설명|
|---|---|
|patience|개선 없이 허용할 에포크 수|
|min_delta|개선으로 인정할 최소 변화량|
|monitor|모니터링 대상 (val_loss, val_accuracy)|
|restore_best_weights|최적 가중치 복원 여부|

### 적용 예시

|라이브러리|방법|
|---|---|
|XGBoost|early_stopping_rounds 파라미터|
|LightGBM|callbacks=[early_stopping()]|
|Keras|EarlyStopping 콜백|

### 장점

| 장점       | 설명            |
| -------- | ------------- |
| 과적합 방지   | 최적 지점에서 학습 중단 |
| 학습 시간 단축 | 불필요한 반복 제거    |
| 자원 절약    | 계산 비용 감소      |