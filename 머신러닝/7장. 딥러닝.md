## 7.1 인공신경망이란?

### 7.1.1 인공신경망 (Artificial Neural Network)

**정의**: 생물학적 뉴런을 모방한 연산 단위들의 네트워크

#### 뉴런 구조

```
입력 (x₁, x₂, ..., xₙ)
        ↓
  가중합: z = Σ(wᵢxᵢ) + b
        ↓
  활성화 함수: a = f(z)
        ↓
      출력 (a)
```

#### 신경망 구조

```
입력층        은닉층         출력층
 (○)         (○)           
 (○) ──────→ (○) ──────→   (○)
 (○)         (○)           
              ↑
         특성 학습
```

|층|역할|
|---|---|
|입력층|데이터 입력 (특성 수 = 노드 수)|
|은닉층|특성 학습 및 변환|
|출력층|최종 예측 (분류: 클래스 수, 회귀: 1개)|

### 7.1.2 인공신경망의 학습

#### 학습 과정

```
1. 순전파 (Forward Propagation)
   입력 → 가중합 → 활성화 → 출력 → 손실 계산
                    ↓
2. 역전파 (Backpropagation)
   손실 → 그래디언트 계산 → 가중치 업데이트
                    ↓
3. 반복 (Epoch)
   수렴할 때까지 1-2 반복
```

#### 경사하강법 (Gradient Descent)

|종류|배치 크기|특징|
|---|---|---|
|Batch GD|전체 데이터|안정적, 느림|
|Stochastic GD|1개|빠름, 불안정|
|Mini-batch GD|일부 (32, 64, ...)|균형 잡힌 방식|

#### 옵티마이저

|옵티마이저|특징|
|---|---|
|SGD|기본, 학습률 고정|
|Momentum|관성 추가, 진동 감소|
|RMSprop|적응적 학습률|
|Adam|Momentum + RMSprop, 가장 널리 사용|

### 7.1.3 인공신경망의 활성화 함수

**목적**: 비선형성 부여 → 복잡한 패턴 학습 가능

|함수|수식|범위|특징|
|---|---|---|---|
|Sigmoid|1/(1+e⁻ˣ)|(0, 1)|기울기 소실, 이진 출력층|
|Tanh|(eˣ-e⁻ˣ)/(eˣ+e⁻ˣ)|(-1, 1)|중심 0, 기울기 소실|
|ReLU|max(0, x)|[0, ∞)|계산 빠름, Dying ReLU|
|Leaky ReLU|max(αx, x)|(-∞, ∞)|Dying ReLU 해결|
|Softmax|eˣⁱ/Σeˣʲ|(0, 1)|다중 분류 출력층, 합=1|

#### 활성화 함수 선택 가이드

```
은닉층 → ReLU (기본), Leaky ReLU
출력층 → 이진 분류: Sigmoid
         다중 분류: Softmax
         회귀: Linear (없음)
```

### 7.1.4 인공신경망의 손실 함수

|문제|손실 함수|설명|
|---|---|---|
|이진 분류|Binary Cross-Entropy|-[y·log(ŷ) + (1-y)·log(1-ŷ)]|
|다중 분류|Categorical Cross-Entropy|-Σyᵢ·log(ŷᵢ)|
|회귀|MSE|Σ(y-ŷ)²/n|
|회귀|MAE|Σ\|y-ŷ\|/n|

## 7.2 인공신경망 구성하기

### 설계 시 결정 사항

|항목|고려 사항|
|---|---|
|입력층|특성 수에 맞게 설정|
|은닉층 수|복잡도에 따라 조절 (보통 1~3개)|
|노드 수|점진적 축소 권장 (512→256→128)|
|활성화 함수|은닉층: ReLU, 출력층: 문제에 맞게|
|손실 함수|문제 유형에 맞게|
|옵티마이저|Adam (기본)|
|학습률|0.001 (기본), 튜닝 필요|

### 기본 구조 예시

```
분류 문제 (MNIST)
─────────────────
Input(784)
    ↓
Dense(256, ReLU)
    ↓
Dense(128, ReLU)
    ↓
Dense(10, Softmax)
```

## 7.3 인공신경망의 성능 개선

### 7.3.1 드롭아웃 (Dropout)

**원리**: 학습 시 무작위로 노드 비활성화 → 과적합 방지

```
훈련 시                    추론 시
○ ─┬─ ○ ─┬─ ○            ○ ─┬─ ○ ─┬─ ○
○ ─┼─ ✗ ─┼─ ○     →     ○ ─┼─ ○ ─┼─ ○
○ ─┼─ ○ ─┼─ ✗            ○ ─┼─ ○ ─┼─ ○
✗ ─┴─ ○ ─┴─ ○            ○ ─┴─ ○ ─┴─ ○

✗ = 비활성화 노드
```

|항목|내용|
|---|---|
|비율|보통 0.2~0.5|
|적용 위치|은닉층 뒤|
|효과|앙상블 효과, 공동 적응 방지|

### 7.3.2 이른 종료 (Early Stop)

**원리**: 검증 손실이 개선되지 않으면 학습 중단

|파라미터|설명|
|---|---|
|monitor|모니터링 대상 (val_loss)|
|patience|개선 없이 대기할 에포크 수|
|restore_best_weights|최적 가중치 복원|

```
검증 손실
    ↑
    │     ╱╲
    │    ╱  ╲____  ← 여기서 중단
    │   ╱
    │  ╱
    └──────────────→ Epoch
           ↑
       patience
```

### 7.3.3 배치 정규화 (Batch Normalization)

**원리**: 각 층의 입력을 정규화하여 내부 공변량 이동 감소

#### 계산 과정

```
1. 배치 평균: μ = Σxᵢ/m
2. 배치 분산: σ² = Σ(xᵢ-μ)²/m
3. 정규화: x̂ = (x-μ)/√(σ²+ε)
4. 스케일/시프트: y = γx̂ + β
```

|효과|설명|
|---|---|
|학습 속도 향상|높은 학습률 사용 가능|
|초기화 민감도 감소|안정적 학습|
|정규화 효과|약간의 과적합 방지|

#### 적용 위치

```
Dense → BatchNorm → Activation → Dropout
```

### 7.3.4 가중치 초기값 설정 (Weight Initialization)

**중요성**: 부적절한 초기화 → 기울기 소실/폭발

|초기화 방법|수식|권장 활성화|
|---|---|---|
|Xavier (Glorot)|W ~ N(0, 2/(nᵢₙ+nₒᵤₜ))|Sigmoid, Tanh|
|He|W ~ N(0, 2/nᵢₙ)|ReLU|
|LeCun|W ~ N(0, 1/nᵢₙ)|SELU|

#### 초기화 선택 가이드

```
활성화 함수
├── Sigmoid/Tanh → Xavier
├── ReLU/Leaky ReLU → He
└── SELU → LeCun
```

### 성능 개선 기법 요약

|기법|주요 효과|적용 시점|
|---|---|---|
|드롭아웃|과적합 방지|은닉층 뒤|
|이른 종료|과적합 방지, 시간 절약|학습 중|
|배치 정규화|학습 안정화, 속도 향상|층 사이|
|가중치 초기화|기울기 문제 해결|학습 전|