## 8.1 CNN(Convolutional Neural Network)의 이해

**정의**: 이미지의 공간적 특성을 추출하는 신경망

### CNN 전체 구조

```
입력 이미지
    ↓
┌─────────────────────┐
│  특성 추출부         │
│  Conv → Pool → ...  │
└─────────────────────┘
    ↓
┌─────────────────────┐
│  분류부             │
│  Flatten → FC → Out │
└─────────────────────┘
```

### 8.1.1 컨볼루션 레이어 (Convolution Layer)

**역할**: 필터(커널)로 이미지의 지역적 특성 추출

#### 컨볼루션 연산

```
입력 (5×5)          필터 (3×3)        특성맵 (3×3)
┌─┬─┬─┬─┬─┐        ┌─┬─┬─┐          ┌─┬─┬─┐
│1│0│1│0│1│        │1│0│1│          │4│3│4│
├─┼─┼─┼─┼─┤   *    ├─┼─┼─┤    =     ├─┼─┼─┤
│0│1│0│1│0│        │0│1│0│          │2│4│3│
├─┼─┼─┼─┼─┤        ├─┼─┼─┤          ├─┼─┼─┤
│1│0│1│0│1│        │1│0│1│          │4│3│4│
├─┼─┼─┼─┼─┤        └─┴─┴─┘          └─┴─┴─┘
│0│1│0│1│0│
├─┼─┼─┼─┼─┤
│1│0│1│0│1│
└─┴─┴─┴─┴─┘
```

#### 주요 용어

|용어|설명|
|---|---|
|필터/커널|학습 가능한 가중치 행렬 (보통 3×3, 5×5)|
|특성맵|컨볼루션 결과 출력|
|스트라이드|필터 이동 간격|
|패딩|입력 가장자리에 값 추가 (same/valid)|
|채널|입력 깊이 (RGB=3, 흑백=1)|

#### 출력 크기 계산

```
출력 크기 = (입력 - 필터 + 2×패딩) / 스트라이드 + 1
```

### 8.1.2 풀링 레이어 (Pooling Layer)

**역할**: 특성맵 크기 축소, 위치 불변성 확보

#### 풀링 종류

```
Max Pooling (2×2)           Average Pooling (2×2)
┌───┬───┐     ┌───┐        ┌───┬───┐     ┌─────┐
│ 1 │ 3 │     │   │        │ 1 │ 3 │     │     │
├───┼───┤ →   │ 4 │        ├───┼───┤ →   │ 2.5 │
│ 2 │ 4 │     │   │        │ 2 │ 4 │     │     │
└───┴───┘     └───┘        └───┴───┘     └─────┘
   최대값 선택                 평균값 계산
```

|종류|특징|
|---|---|
|Max Pooling|가장 강한 특성 추출, 가장 많이 사용|
|Average Pooling|전체 특성 평균화|
|Global Average Pooling|채널당 1개 값, FC 대체 가능|

### 8.1.3 완전 연결 레이어 (Fully Connected Layer)

**역할**: 추출된 특성을 기반으로 최종 분류

```
특성맵 (7×7×512)
       ↓
    Flatten
       ↓
 1D 벡터 (25088)
       ↓
   FC (4096)
       ↓
   FC (1000)
       ↓
   Softmax
       ↓
   클래스 확률
```

## 8.2 CNN 구성하기

### 기본 구성 패턴

```
[Conv → ReLU → Pool] × N → Flatten → [FC → ReLU] × M → Output

전형적인 구조:
Input (224×224×3)
    ↓
Conv(32, 3×3) → ReLU → MaxPool(2×2)
    ↓
Conv(64, 3×3) → ReLU → MaxPool(2×2)
    ↓
Conv(128, 3×3) → ReLU → MaxPool(2×2)
    ↓
Flatten → FC(256) → ReLU → Dropout → FC(10) → Softmax
```

### 설계 원칙

|원칙|설명|
|---|---|
|필터 수 증가|깊어질수록 필터 수 증가 (32→64→128)|
|크기 감소|풀링으로 공간 크기 점진 축소|
|작은 필터|3×3 필터 선호 (큰 필터 대체 가능)|
|배치 정규화|Conv 후 적용으로 학습 안정화|

## 8.3 이미지 분류 응용

### 8.3.1 이미지란?

|구성|설명|
|---|---|
|픽셀|이미지의 최소 단위|
|해상도|가로 × 세로 픽셀 수|
|채널|색상 정보 (RGB=3, 흑백=1)|
|텐서 표현|(높이, 너비, 채널) 또는 (배치, 채널, 높이, 너비)|

### 8.3.2 이미지 데이터 전처리

|기법|설명|
|---|---|
|리사이즈|모델 입력 크기에 맞춤 (224×224 등)|
|정규화|픽셀값 0~1 또는 표준화|
|데이터 증강|회전, 반전, 크롭, 색상 변환 등|

#### 데이터 증강 예시

```
원본 이미지
    ↓
┌───────────────────────────────┐
│ 수평 반전 │ 회전 │ 크롭 │ 밝기 │
└───────────────────────────────┘
    ↓
학습 데이터 다양성 증가 → 과적합 방지
```

### 8.3.3 이미지넷 (ImageNet)

|항목|내용|
|---|---|
|데이터셋|1400만+ 이미지, 1000개 클래스|
|대회|ILSVRC (2010~2017)|
|의의|딥러닝 혁명의 시작점, 벤치마크 표준|

### 8.3.4 VGGNet (2014)

**특징**: 3×3 작은 필터만 사용, 깊고 단순한 구조

```
VGG16 구조
──────────
[Conv3-64] × 2 → Pool
[Conv3-128] × 2 → Pool
[Conv3-256] × 3 → Pool
[Conv3-512] × 3 → Pool
[Conv3-512] × 3 → Pool
FC-4096 → FC-4096 → FC-1000
```

|장점|단점|
|---|---|
|단순한 구조|파라미터 많음 (138M)|
|전이 학습에 효과적|느린 학습 속도|

### 8.3.5 GoogLeNet/Inception (2014)

**특징**: 인셉션 모듈로 다양한 스케일의 특성 추출

```
Inception 모듈
─────────────
        입력
    ┌────┼────┬────┐
    ↓    ↓    ↓    ↓
  1×1  1×1  1×1  Pool
        ↓    ↓    ↓
       3×3  5×5  1×1
    └────┴────┴────┘
           ↓
      Concat (채널 방향)
```

|특징|설명|
|---|---|
|1×1 Conv|차원 축소, 연산량 감소|
|병렬 경로|다양한 수용 영역|
|Global Avg Pool|FC 파라미터 대폭 감소|

### 8.3.6 ResNet (2015)

**핵심**: 잔차 연결(Skip Connection)로 깊은 네트워크 학습 가능

```
Residual Block
──────────────
    입력 x
    ├─────────────┐
    ↓             │
  Conv → BN → ReLU│
    ↓             │
  Conv → BN       │
    ↓             │
   (+) ←──────────┘  x (identity)
    ↓
   ReLU
    ↓
  출력: F(x) + x
```

|버전|층 수|특징|
|---|---|---|
|ResNet-18|18|경량 모델|
|ResNet-50|50|가장 널리 사용|
|ResNet-152|152|높은 정확도|

**장점**: 기울기 소실 해결, 1000층 이상 학습 가능

### 8.3.7 EfficientNet (2019)

**핵심**: 깊이, 너비, 해상도를 복합 스케일링

```
복합 스케일링
────────────
       기본 모델 (B0)
           ↓
    ┌──────┼──────┐
    ↓      ↓      ↓
  깊이↑  너비↑  해상도↑
    └──────┼──────┘
           ↓
       B1, B2, ..., B7
```

|버전|특징|
|---|---|
|B0|기본, 경량|
|B4|정확도/효율 균형|
|B7|최고 정확도|

**장점**: 적은 파라미터로 높은 성능

### 8.3.8 전이 학습 (Transfer Learning)

**정의**: 사전 학습된 모델을 새 작업에 활용

```
전이 학습 방식
─────────────
사전학습 모델 (ImageNet)
┌─────────────────────┐
│  특성 추출부 (동결)   │ ← 기존 가중치 유지
└─────────────────────┘
          ↓
┌─────────────────────┐
│  새로운 분류부       │ ← 새로 학습
│  (Custom FC Layer)  │
└─────────────────────┘
```

#### 전이 학습 전략

|전략|데이터 양|유사도|방법|
|---|---|---|---|
|Feature Extraction|적음|높음|전체 동결, 분류부만 학습|
|Fine-tuning (일부)|중간|중간|상위층 일부 학습|
|Fine-tuning (전체)|많음|낮음|전체 재학습 (낮은 학습률)|

#### 장점

|장점|설명|
|---|---|
|적은 데이터로 학습|사전학습된 특성 활용|
|빠른 수렴|좋은 초기값에서 시작|
|높은 성능|대규모 데이터셋의 지식 전이|